{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"LLM - ChatGPT applied on Specific Context\"\n",
        "---"
      ],
      "id": "77a0f089"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "<p>\n",
        "Devido aos avanços da IA Generativa com os LLMs, o ChatGPT, desenvolvido pela OpenAI, tornou-se uma ferramenta popular para geração de texto. Através da interface em <https://chat.openai.com/>, os usuários podem interagir com o ChatGPT para receber respostas, resumos de texto e traduções. No entanto, para responder a perguntas específicas de contexto inédito, como as baseadas em um documento PDF, é geralmente necessário usar a API do ChatGPT. \n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Para simplificar essa tarefa, surgiu o LangChain <https://www.langchain.com/>, uma estrutura de código aberto que facilita o desenvolvimento de aplicativos com LLMs. O LangChain atua como uma interface genérica para diferentes LLMs, permitindo a construção de aplicativos LLM integrados a fontes de dados externas.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Este projeto visa demonstrar a aplicação prática do ChatGPT em um contexto específico com o auxílio do LangChain, explorando cada etapa e os principais desafios envolvidos.\n",
        "</p>\n",
        "\n",
        "# Desenvolvimento\n",
        "\n",
        "## Setup and Constants:\n",
        "\n",
        "Importando todos os módulos necessários os quais já foram previamente instalados."
      ],
      "id": "09ff98e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false\n",
        "\n",
        "# Setup\n",
        "from langchain.text_splitter             import RecursiveCharacterTextSplitter # Split - Tokenization\n",
        "from langchain.embeddings                import OpenAIEmbeddings               # Embeddings\n",
        "from langchain.vectorstores              import Chroma                         # Vector Store\n",
        "from langchain.llms                      import OpenAI                         # Models\n",
        "from langchain.chat_models               import ChatOpenAI                     # Chat\n",
        "from langchain.chains.question_answering import load_qa_chain                  # QA\n",
        "from langchain.callbacks                 import get_openai_callback            # Callback\n",
        "import os\n",
        "import textract\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_columns', None) "
      ],
      "id": "118200da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definindo as constantes que serão utilizadas ao longo do código."
      ],
      "id": "d38413ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Constants\n",
        "OPENAI_API_KEY       = os.environ.get('OPENAI_API_KEY')        # Definida como variável de ambiente\n",
        "FILE_PATH_CONTEXT    = \"./source/project1/context/ValeskaAlves.pdf\" # Path para o arquivo de contexto\n",
        "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"                # Modelo para o embedding do contexto\n",
        "FILE_PATH_DB         = \"./source/project1/chroma/\"             # Path para Vector Store\n",
        "MODEL_NAME           = \"gpt-3.5-turbo\"                         # Modelo para responder as perguntas"
      ],
      "id": "2ded66a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Loading\n",
        "Aqui carregamos o arquivo de contexto, que pode estar em qualquer formato (<https://textract.readthedocs.io/en/stable/>) e convertemos em uma string."
      ],
      "id": "199f712d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Document Loading\n",
        "file_path = FILE_PATH_CONTEXT\n",
        "doc       = textract.process(file_path)\n",
        "text      = doc.decode('utf-8')"
      ],
      "id": "25aab6b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Splitting - Tokenization\n",
        "Nesta etapa, nosso objetivo é segmentar o contexto em vários documentos, com a principal ideia de dividir o texto em unidades semanticamente relevantes.\n",
        "\n",
        "Por simplicidade, essa divisão será feita considerando um número máximo de caracteres por documento, levando em conta a estrutura textual original, ou seja, espaços em branco, quebras de linhas e parágrafos podem ser considerados como separadores aqui. \n",
        "\n",
        "Existem diversas técnicas para efetuar este processo, estamos usando apenas uma delas.\n"
      ],
      "id": "0ad0dd82"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Document Splitting - Tokenization\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size      = 512, # Quantidade máxima de caracteres por split\n",
        "    chunk_overlap   = 24,  # Quantidade de máxima de caracteres sobrepostos por split\n",
        ")\n",
        "\n",
        "chunks = text_splitter.create_documents([text])"
      ],
      "id": "bc560bc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embeddings and Vector Stores\n",
        "\n",
        "***Embeddings***:\n",
        "\n",
        "Cada \"chunk\" gerado na etapa anterior é utilizado para construir uma representação numérica do texto em formato de vetor. Dessa forma, textos com conteúdo semanticamente semelhante terão vetores semelhantes no \"embedding space\" (espaço de incorporação). Assim, podemos comparar embeddings (vetores) e encontrar textos semelhantes.\n",
        "\n",
        "Observações:\n",
        "\n",
        "- O \"chunking\" (divisão realizada na etapa anterior) é o processo de dividir o texto em pedaços ou \"chunks\" semânticos, que podem ser frases, parágrafos ou outras unidades maiores. No entanto, esses chunks podem conter múltiplos tokens.\n",
        "\n",
        "- O número de tokens nem sempre é igual ao número de embeddings. Embora cada token geralmente corresponda a um embedding, um único token às vezes pode corresponder a múltiplos embeddings, especialmente em casos onde é utilizada a tokenização de subpalavras. Além disso, pode haver tokens que não possuem embeddings correspondentes, dependendo da cobertura do vocabulário do modelo de embedding utilizado. Portanto, embora haja frequentemente uma forte correlação entre o número de tokens e o número de embeddings, eles nem sempre são exatamente iguais.\n",
        "\n",
        "***Vector Stores***:\n",
        "\n",
        "Um \"armazenamento de vetores\" é um banco de dados onde é possível pesquisar facilmente vetores semelhantes posteriormente. Isso se torna útil quando tentamos encontrar documentos relevantes para uma questão. Nesta etapa, vamos armazenar os embeddings em um banco de dados NoSQL.\n"
      ],
      "id": "d916dc24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Custom Embeddings Function\n",
        "class CustomOpenAIEmbeddings:\n",
        "    def __init__(self, api_key, model_name):\n",
        "        self.api_key = api_key\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # Assume inputs is a list of texts and we're querying OpenAI API for embeddings\n",
        "        responses = openai.Embedding.create(\n",
        "            input=inputs,\n",
        "            model=self.model_name,\n",
        "            user=self.api_key\n",
        "        )\n",
        "        # Extract embeddings from the responses\n",
        "        return [response['data'] for response in responses['data']]\n",
        "\n",
        "# Using the custom embedding function\n",
        "embeddings = CustomOpenAIEmbeddings(\n",
        "    api_key=OPENAI_API_KEY, \n",
        "    model_name=EMBEDDING_MODEL_NAME\n",
        ")\n",
        "\n",
        "# Vector Stores\n",
        "persist_directory = FILE_PATH_DB \n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=chunks, \n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ],
      "id": "fbcc6461",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval\n",
        "\n",
        "Na fase de recuperação (retrieval), buscamos nos embeddings armazenados no banco de dados os mais relevantes para responder à pergunta que estamos formulando. Em outras palavras, procuramos identificar os trechos de texto que podem conter informações necessárias para responder à pergunta, pois são esses trechos que serão enviados para o ChatGPT junto com a pergunta, em vez de todo o contexto.\n",
        "\n",
        "A recuperação é um dos principais desafios enfrentados ao tentar responder perguntas com base em nossos documentos. Muitas vezes, quando a resposta à nossa pergunta não é satisfatória, isso se deve a um problema na fase de recuperação. Para lidar com isso, existem várias técnicas disponíveis, e adotaremos a seguinte abordagem:\n",
        "\n",
        "***Maximum Marginal Relevance(MMR)***: Essa técnica seleciona os ***fetch_k*** embeddings mais relevantes para responder à pergunta e, dentre esses, escolhe os ***k*** mais relevantes e diversos entre si. Isso ajuda a contornar problemas de recuperação de tópicos duplicados, caso ocorram.\n",
        "\n",
        "Nessa etapa, é crucial definir nossa pergunta em relação ao contexto. No presente caso, vamos utilizar um currículo profissional (apresentado logo abaixo) e formular a seguinte questão: ***Qual o nome deste candidato ?***\n",
        "\n",
        "![](./source/project1/context/cvPedro.jpg){fig-align=\"center\"}\n"
      ],
      "id": "abe5724b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Retrieval\n",
        "query = \"Qual o nome deste candidato ?\"\n",
        "\n",
        "# Maximum Marginal Relevance(MMR)\n",
        "docs  = vectordb.max_marginal_relevance_search(query, k= 2 , fetch_k= 5)"
      ],
      "id": "07c6442e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question Answering\n",
        "Agora que temos os documentos mais relevantes para responder à nossa questão, obtidos na fase anterior de recuperação, procedemos a passar esses documentos juntamente com a pergunta original para um modelo de linguagem (ChatGPT), a fim de obter uma resposta à nossa consulta.\n"
      ],
      "id": "ea8f2a80"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Question Answering\n",
        "\n",
        "llm   = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    r = chain.run(input_documents=docs, question=query)\n",
        "    print(r)\n",
        "    print(\"\\n\")\n",
        "    #print(cb)"
      ],
      "id": "c7914c46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Context 1\n",
        "Fazendo perguntas ainda dentro do mesmo contexto anterior (currículo profissional)\n",
        "\n",
        "Instanciando o modelo"
      ],
      "id": "d590aaf2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = AskAboutContext(\n",
        "    key                  = OPENAI_API_KEY, \n",
        "    file_path_context    = FILE_PATH_CONTEXT , \n",
        "    file_path_db         = FILE_PATH_DB,\n",
        "    embedding_model_name = EMBEDDING_MODEL_NAME,\n",
        "    model_name           = MODEL_NAME,\n",
        "    )\n",
        "\n",
        "model.fit()"
      ],
      "id": "681831bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pergunta 1"
      ],
      "id": "58a067f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.query(\"Qual a cidade deste candidato?\")"
      ],
      "id": "b3f33020",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pergunta 2"
      ],
      "id": "e4865b0a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.query(\"Qual o objetivo deste candidato?\") "
      ],
      "id": "388e7791",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Context 2\n",
        "\n",
        "Faremos agora perguntas sobre a tabela de preços de veículos logo abaixo.\n",
        "\n",
        "![](./source/project1/context/precos.jpg){fig-align=\"center\"}\n"
      ],
      "id": "d9c1d927"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = AskAboutContext(\n",
        "    key                  = OPENAI_API_KEY, \n",
        "    file_path_context    = \"./source/project1/context/precos.jpg\" , \n",
        "    file_path_db         = FILE_PATH_DB,\n",
        "    embedding_model_name = EMBEDDING_MODEL_NAME,\n",
        "    model_name           = MODEL_NAME,\n",
        "    )\n",
        "\n",
        "model.fit()"
      ],
      "id": "c27bcdf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pergunta 1"
      ],
      "id": "aaf8be0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.query(\"Quais são os modelos de carros existentes nessa tabela?\")"
      ],
      "id": "b1ec1a3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pergunta 2"
      ],
      "id": "5d5f9580"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.query(\"Qual foi o modelo mais barato no ano de 2016?\") "
      ],
      "id": "a8328a3e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}