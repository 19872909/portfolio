[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I have over five years of experience in Business Intelligence and Data Science, having collaborated with international corporations such as Samsung, InBev, Bayer, and Honda across various business domains including marketing, operations management, and customer experience.\nAs a Data Scientist, I have worked within cross-functional teams to deliver data-driven solutions, establish data governance frameworks, and utilize advanced analytics to extract insights.\nI am studying Behavioral Data Science at the Universitat de Barcelona. This program is enhancing my skills in data analysis, statistical modeling, machine learning, deep learning, and data visualization using R and Python. It also gives me an understanding of psychology and behavioral economics, focusing on psychographics, predictive behavioral modeling, and psychometrics. This educational experience is enabling me to apply data science in ways that are more empathetic and impactful.\n\nBS in Electrical Engineering-Telecommunication - PUC Campinas, SP - 2011\nSpecialization in Complex Data Mining - Unicamp, SP - 2018\nSpecialization in Computer Software Engineering - Unicamp, SP - 2013\nSpecialization in Data Science (Big Data Processing and Analytics) - Mackenzie, SP - 2021\nMaster Behavioral Data Science - Barcelona University - 2024"
  },
  {
    "objectID": "index.html#my-portfolio",
    "href": "index.html#my-portfolio",
    "title": "Home",
    "section": "",
    "text": "I have over five years of experience in Business Intelligence and Data Science, having collaborated with international corporations such as Samsung, InBev, Bayer, and Honda across various business domains including marketing, operations management, and customer experience.\nAs a Data Scientist, I have worked within cross-functional teams to deliver data-driven solutions, establish data governance frameworks, and utilize advanced analytics to extract insights.\nI am studying Behavioral Data Science at the Universitat de Barcelona. This program is enhancing my skills in data analysis, statistical modeling, machine learning, deep learning, and data visualization using R and Python. It also gives me an understanding of psychology and behavioral economics, focusing on psychographics, predictive behavioral modeling, and psychometrics. This educational experience is enabling me to apply data science in ways that are more empathetic and impactful.\n\nBS in Electrical Engineering-Telecommunication - PUC Campinas, SP - 2011\nSpecialization in Complex Data Mining - Unicamp, SP - 2018\nSpecialization in Computer Software Engineering - Unicamp, SP - 2013\nSpecialization in Data Science (Big Data Processing and Analytics) - Mackenzie, SP - 2021\nMaster Behavioral Data Science - Barcelona University - 2024"
  },
  {
    "objectID": "index.html#tools",
    "href": "index.html#tools",
    "title": "Home",
    "section": "Tools",
    "text": "Tools"
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Project: PIMA Indians Diabetes",
    "section": "",
    "text": "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases.",
    "crumbs": [
      "Projects",
      "Project: PIMA Indians Diabetes"
    ]
  },
  {
    "objectID": "project1.html#correlations",
    "href": "project1.html#correlations",
    "title": "Project: PIMA Indians Diabetes",
    "section": "Correlations",
    "text": "Correlations\n\nEach square shows the correlation between the variables on each axis. Correlation ranges from -1 to +1.\n\nValues closer to 0: There is no linear trend between the two variables.\nValues closer to 1: The correlation is positive; that is as one increases so does the other and the closer to 1 the stronger this relationship is.\nValues closer to -1: One variable will decrease as the other increases\n\nThe diagonals are all 1/dark because those squares are correlating each variable to itself (so it’s a perfect correlation). For the rest the larger the number and darker the color the higher the correlation between the two variables.\n\n\n\nCorrelation matrix:\nThere is a strong positve correlation between Pregnancies and Age, Glucose and Outcome. There is a strong negative correlation between Skinthickness and Age, Pregnancies and SkinThickness, Pregnance and Insulin\n\n#Correlation of features with the Outcome\ncorr_report = pima.corr()['Outcome']\ncorr_report.sort_values(ascending=False)\n\nOutcome                     1.000000\nGlucose                     0.466581\nBMI                         0.292695\nAge                         0.238356\nPregnancies                 0.221898\nDiabetesPedigreeFunction    0.173844\nInsulin                     0.130548\nSkinThickness               0.074752\nBloodPressure               0.065068\nName: Outcome, dtype: float64\n\n\n\noutliers = pima[pima['Glucose'] &gt; 150]\noutliers\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n8\n2\n197\n70\n45\n543\n30.5\n0.158\n53\n1\n\n\n11\n10\n168\n74\n0\n0\n38.0\n0.537\n34\n1\n\n\n13\n1\n189\n60\n23\n846\n30.1\n0.398\n59\n1\n\n\n14\n5\n166\n72\n19\n175\n25.8\n0.587\n51\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n749\n6\n162\n62\n0\n0\n24.3\n0.178\n50\n1\n\n\n753\n0\n181\n88\n44\n510\n43.3\n0.222\n26\n1\n\n\n754\n8\n154\n78\n32\n0\n32.4\n0.443\n45\n1\n\n\n759\n6\n190\n92\n0\n0\n35.5\n0.278\n66\n1\n\n\n761\n9\n170\n74\n31\n0\n44.0\n0.403\n43\n1\n\n\n\n\n140 rows × 9 columns\n\n\n\n\n\noutliers['Outcome'].value_counts()\n\nOutcome\n1    105\n0     35\nName: count, dtype: int64\n\n\n\n# Histogram to analyze the distribution of data\nfig, ax = plt.subplots(ncols=3, nrows=3, figsize=(15,12))\nindex = 0\nax = ax.flatten()\n\nfor col, value in pima.items():\n    col_dist = sns.histplot(value, ax=ax[index], kde=True, stat=\"density\", linewidth=0)\n    col_dist.set_xlabel(col,fontsize=10)\n    col_dist.set_ylabel('density',fontsize=10)\n    index += 1\nplt.tight_layout(pad=0.8, w_pad=0.5, h_pad=5.0)\n\n\n\n\n\n\n\n\n\n# Boxplot to analyze the distribution of data\nfig, ax = plt.subplots(ncols=3, nrows=3, figsize=(15,12))\nindex = 0\nax = ax.flatten()\n\nfor col, value in pima.items():\n    col_dist = sns.boxplot(value, ax=ax[index])\n    col_dist.set_xlabel(col,fontsize=10)\n    col_dist.set_ylabel('density',fontsize=10)\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)\n\n\n\n\n\n\n\n\n\n\n\nOutliers\nThere are some outliers analyzed through the distribution of data in the histograms and boxplots.\nThey following code has removed the outliers before to proceed with the ML model.\n\nIQR - Interquartile Range\nThe lower quartile corresponds with the 25th percentile and the upper quartile corresponds with the 75th percentile, so IQR = Q3 − Q1.\nThe IQR is an example of a trimmed estimator, defined as the 25% trimmed range, which enhances the accuracy of dataset statistics by dropping lower contribution, outlying points.\n\nQ1 = pima.quantile(0.25)\nQ3 = pima.quantile(0.75)\n\n\nIQR = Q3 - Q1\nIQR\n\nPregnancies                   5.0000\nGlucose                      41.2500\nBloodPressure                18.0000\nSkinThickness                32.0000\nInsulin                     127.2500\nBMI                           9.3000\nDiabetesPedigreeFunction      0.3825\nAge                          17.0000\nOutcome                       1.0000\ndtype: float64\n\n\n\npima.shape\n\n(768, 9)\n\n\n\nThe interquartile range is often used to find outliers in data. Outliers here are defined as observations that fall below Q1 − 1.5 IQR or above Q3 + 1.5 IQR. In a boxplot, the highest and lowest occurring value within this limit are indicated by whiskers of the box (frequently with an additional bar at the end of the whisker) and any outliers as individual points.\n\n\n# Outlier removal\npima = pima[~((pima &lt; (Q1 - 1.5 * IQR)) | (pima &gt; (Q3 + 1.5 * IQR))).any(axis = 1)]\npima.shape\n\n(639, 9)\n\n\n\ncount = pima[\"Outcome\"].value_counts()\ncount\n\nOutcome\n0    439\n1    200\nName: count, dtype: int64\n\n\n\n# Data distribution\nfig, ax = plt.subplots(ncols=3, nrows=3, figsize=(15,12))\nindex = 0\nax = ax.flatten()\n\nfor col, value in pima.items():\n    col_dist = sns.boxplot(value, ax=ax[index])\n    col_dist.set_xlabel(col,fontsize=10)\n    col_dist.set_ylabel('density',fontsize=10)\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)\n\n\n\n\n\n\n\n\n\n# Histogram to analyze the distribution of data\nfig, ax = plt.subplots(ncols=3, nrows=3, figsize=(15,12))\nindex = 0\nax = ax.flatten()\n\nfor col, value in pima.items():\n    col_dist = sns.histplot(value, ax=ax[index], kde=True, stat=\"density\", linewidth=0)\n    col_dist.set_xlabel(col,fontsize=10)\n    col_dist.set_ylabel('density',fontsize=10)\n    index += 1\nplt.tight_layout(pad=0.8, w_pad=0.5, h_pad=5.0)\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression ML Model\n\n# Define x and y\nfeature_cols =['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\nx=pima[feature_cols]\ny=pima.Outcome\n\n\n#split x and y into training (70%) and testing (30%) sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=0)\n\n\n\n\nStandardize features\nStandardize features by removing the mean and scaling to unit variance.\nThe standard score of a sample x is calculated as: z = (x - u) / s, where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\n\nscale = StandardScaler()\nx_train = scale.fit_transform(x_train)\nx_test = scale.fit_transform(x_test)\n\n\n#Logistic regression model fit on the training set\nlogreg = LogisticRegression(solver='lbfgs', max_iter=3000)\nlogreg.fit(x_train, y_train)\n\nLogisticRegression(max_iter=3000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=3000)\n\n\n\n#Using the trained model to predict the outcome for samples in x_test.\ny_pred = logreg.predict(x_test)\n\n\n#Return the probability estimates.\ny_score = logreg.predict_proba(x_test)[:, 1]\n\n\n\n\nML Model Assessment\n\nPrecision: Percentage of correct positive predictions relative to total positive predictions.\nRecall: Percentage of correct positive predictions relative to total actual positives.\nF1 Score: A weighted harmonic mean of precision and recall. The closer to 1, the better the model. F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n\n\n#Comparing actual result and predicted result\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.78      0.93      0.85       127\n           1       0.78      0.48      0.59        65\n\n    accuracy                           0.78       192\n   macro avg       0.78      0.70      0.72       192\nweighted avg       0.78      0.78      0.76       192\n\n\n\n\nPrecision: Out of all the patients that the model predicted would get diabetes, 78% actually did.\nRecall: Out of all the patients that actually did get diabetes, the model predicted this outcome correctly for 48% of those patients.\nF1 Score: 0.59 - Since this value is close to 1, it tells us that the model does a good job of predicting whether or not patients will get diabetes. 2 * (Precision * Recall) / (Precision + Recall) ***** 2 * (.78 * .48) / (.78 + .48)\nSupport: These values is regarding how many patients belonged to each class in the test dataset. Among the patientis in the test dataset, 127 did not get diabetes and 65 did get diabetes.\n\n\n#The lower the RMS value, the better. O means the model is perfect.\nrms = mean_squared_error(y_test, y_pred, squared=False)\nrms\n\n0.4732423621500228\n\n\n\n\n\nROC Curve\nThe ROC curve shows the trade-off between sensitivity (or True Positve Rate) and specificity (1 – False Positive Rate).\nClassifiers that give curves closer to the top-left corner indicate a better performance\nROC/AUC does not require to set a classification threshold and it’s still useful when there is high class imbalance\n\nROC curve can help you to choose a threshold that balances sensitivity and specificity in a way that makes sense for your particular context\nYou can’t actually see the thresholds used to generate the curve on the ROC curve itself\n\n\nfpr, tpr, thresh = roc_curve(y_test, y_score, pos_label=logreg.classes_[1])\n\n\n#AUC is the percentage of the ROC plot that is underneath the curve:\n# IMPORTANT: first argument is true values, second argument is predicted probabilities\nprint(metrics.roc_auc_score(y_test, y_pred))\n\n0.7030284675953968\n\n\n\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, linewidth=1, marker='.', label='Logistic')\nplt.plot([0,1], [0,1], linestyle='--', label='No Skill')\nplt.rcParams['font.size'] = 8\nplt.title('ROC curve')\nplt.xlabel('Specificity (FPR)')\nplt.ylabel('Sensitivity (TPR)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAccuracy\nPercentage of correct predictions\n\nprint('Accuracy of Logistic regression model is {}'.format(accuracy_score(y_test,y_pred)))\n\nAccuracy of Logistic regression model is 0.7760416666666666\n\n\n\n\nNull accuracy\nNull accuracy refers to the accuracy that could be achieved by always predicting the most frequent class in the dataset.\nIn the test set, 66% (127) of patients did not have diabetes, while 34% did. In this case, the null accuracy would be 66%, because if we always predicted “not diabetes,” we would be correct 66% of the time.\n\ny_test.value_counts().head(1) / len(y_test)\n\nOutcome\n0    0.661458\nName: count, dtype: float64\n\n\nAs the null accuracy is less than model accuracy, it indicates a good result.\n\nComparing the true and predicted response values\n\n#print the 25 first true and predict responses\nprint ('True:'), y_test.values[0:25]\n\nTrue:\n\n\n(None,\n array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n        0, 0, 0]))\n\n\n\nprint ('Pred:'), y_pred[0:25]\n\nPred:\n\n\n(None,\n array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n        0, 0, 1]))\n\n\n\n\n\nConfusion Matrix\nConfusion matrix allows to calculate a variety of metrics. It’s a useful for multi-class problems (more than two response classes)\n\nEvery observation in the testing set is represented in exactly one box\nIt’s 2x2 matrix because there are 2 responses classes\nThe format shown here is not universal\n\nBasic Terminology\n\nTP: correctly predict that they have diabetes\nTN: correctly predict that they do not have diabetes\nFP: incorrectly predict that they do have diabetes\nFN: incorrectly predict that they do not have diabetes\n\n\n#IMPORTANT: First argument is true values, second argument is predict values\nmetrics.confusion_matrix(y_test, y_pred)\n\narray([[118,   9],\n       [ 34,  31]])\n\n\n\n#Graphic visualization\ncm = confusion_matrix(y_test, y_pred, labels=logreg.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nTN: 118 patients without diabetes were correctly predicted as no diabetics\nFP: 9 patients without diabetes were incorrectly predicted as diabetics\nFN: 34 patients with diabetes were incorrectly predicted as no diabetics\nTP: 31 patients with diabetes were correctly predicted as diabetics\n\n\n#save confusion matrix and slice into four pieces\nconfusion = metrics.confusion_matrix(y_test, y_pred)\nTN = confusion[0,0]\nFP = confusion[0,1]\nFN = confusion[1,0]\nTP = confusion[1,1]\nconfusion\n\narray([[118,   9],\n       [ 34,  31]])\n\n\n\nMetrics computed from a confusion matrix\n\n#How often the classifier is correct?\nprint('Accuracy: {}'.format((TP + TN) / (TP + TN + FP + FN)))\n#print('Accuracy: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))\n\nAccuracy: 0.7760416666666666\n\n\n\n\nk-Fold Cross-Validation\nThe k-fold cross-validation procedure divides a limited dataset into k non-overlapping folds. Each of the k folds is given an opportunity to be used as a held back test set, whilst all other folds collectively are used as a training dataset. A total of k models are fit and evaluated on the k hold-out test sets and the mean performance is reported.\n\n#This will give the overall accuracy of your model .\nkf = KFold(n_splits=10, random_state=1, shuffle=True)\ncv_result = cross_val_score(logreg, x, y, cv=kf, scoring='accuracy', n_jobs=-1)\nprint('Accuracy: %.3f (%.3f)' % (mean(cv_result), std(cv_result)))\n\nAccuracy: 0.784 (0.033)\n\n\n\n#How often the classifier is incorrect?\nprint('Error: {}'.format((FP + FN)/(TP + TN + FP + FN)))\n#print('Error: {}'.format(1-metrics.accuracy_score(y_test, y_pred_class)))\n\nError: 0.22395833333333334\n\n\n\n#When the actual value is positive, how often is the prediction correct?\nprint('Sensitivity (TPR): {}'.format(TP / (TP + FN)))\n#metrics.recall_score(y_test, y_pred_class)\n\nSensitivity (TPR): 0.47692307692307695\n\n\n\n#When the actual value is negative, how often is the prediction correct?\nprint('Specificity (FPR): {}'.format(TN / (TN + FP))) \n#recall_score(y_test, y_pred_class, pos_label=0)\n\nSpecificity (FPR): 0.9291338582677166\n\n\n\n#When a positive value is predicted, how often is the prediction correct?\nprint('Precision: {}'.format(TP / (TP + FP))) \n#metrics.precision_score(y_test, y_pred_class)\n\nPrecision: 0.775\n\n\n\n\n\nResult\nThe choice of metrics depends on the business objective.\nFor this project, sensibility (False Negative) is the metric most important, since predicting diabetics as no diabetics is the worst expected error. This may imply no further investigations and consequently no treatment of the disease.\nSo the better result is to have a Sensitivity (The correct prediction for positive values) result higher than a Specificity (The correct prediction for negative values) result.\nThe error in predicting a healthy patient as a diabetic patient is more acceptable than the opposite.\n\n\nAdjusting the classification threshold\n\n#print the first 10 predicted responses\nlogreg.predict(x_test)[0:10]\n\narray([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n\n\n\n#print the first 10 predicted probabilities of class membership\nlogreg.predict_proba(x_test)[0:10, :]\n\narray([[0.83358342, 0.16641658],\n       [0.80138151, 0.19861849],\n       [0.95231282, 0.04768718],\n       [0.87348851, 0.12651149],\n       [0.67158939, 0.32841061],\n       [0.38292382, 0.61707618],\n       [0.93226643, 0.06773357],\n       [0.5219636 , 0.4780364 ],\n       [0.92950252, 0.07049748],\n       [0.69118097, 0.30881903]])\n\n\n\n#print the first 10 predicted probabilities for class 1\nlogreg.predict_proba(x_test)[0:10, 1]\n\narray([0.16641658, 0.19861849, 0.04768718, 0.12651149, 0.32841061,\n       0.61707618, 0.06773357, 0.4780364 , 0.07049748, 0.30881903])\n\n\n\nfig = sns.histplot(data=y_score, bins=8)\nplt.xlabel(\"Predicted probability of diabetes\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of predicted probabilities\") \nplt.xlim(0,1)\nplt.rcParams['font.size']=10\nplt.show(fig)\n\n\n\n\n\n\n\n\n\nFinding the best threshold for predicting diabetes and to increase the sensitivity of the classifier\nThreshold of 0.5 is used by default (for binary problems) to convert predicted possibilities into class predictions Threshold can be adjusted to increase sensitivity or specificity Sensitivity and specificity have an inverse relationship\n\n# calculate roc curves\nfpr, tpr, thresholds = roc_curve(y_test, y_score)\n\n\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n\nBest Threshold=0.242365, G-Mean=0.796\n\n\n\n# get the best threshold\nJ = tpr - fpr\nix = argmax(J)\nbest_thresh = thresholds[ix]\nprint('Best Threshold=%f' % (best_thresh))\n\nBest Threshold=0.242365\n\n\n\n# define a function that accepts a threshold and prints sensitivity and specificity\ndef evaluate_threshold(threshold):\n    print('Sensitivity:', tpr[thresholds &gt; threshold][-1])\n    print('Specificity:', 1 - fpr[thresholds &gt; threshold][-1])\n\n\nevaluate_threshold(0.5)\n\nSensitivity: 0.47692307692307695\nSpecificity: 0.9291338582677166\n\n\n\nevaluate_threshold(0.3)\n\nSensitivity: 0.7230769230769231\nSpecificity: 0.7795275590551181\n\n\n\nevaluate_threshold(0.242365)\n\nSensitivity: 0.8461538461538461\nSpecificity: 0.7480314960629921\n\n\n\n#predict diabetes if the predicted probabilities is greater than 0.242365\ny_pred_2 = preprocessing.binarize([y_score], threshold=0.242365)[0]\n\n\n#print the first 10 predicted probabilites\ny_pred[0:10]\n\narray([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n\n\n\n#print the first 10 predicted classes with the lower threshold\ny_pred_2[0:10]\n\narray([0., 0., 0., 0., 1., 1., 0., 1., 0., 1.])\n\n\n\n#previous confusion matrix (default threshold of 0.5)\nconfusion\n\narray([[118,   9],\n       [ 34,  31]])\n\n\n\n#now confusion matrix (threshold of 0.3)\nconfusion_2 = metrics.confusion_matrix(y_test, y_pred_2)\nTN = confusion_2[0,0]\nFP = confusion_2[0,1]\nFN = confusion_2[1,0]\nTP = confusion_2[1,1]\n\n\nconfusion_2\n\narray([[95, 32],\n       [10, 55]])\n\n\n\n#sensitivity has increased (used to be 0.7384615384615385)\nprint('Sensitivity (TPR): {}'.format(TP / (TP + FN)))\n\nSensitivity (TPR): 0.8461538461538461\n\n\n\n#sensitivity has increased (used to be 0.7795275590551181)\nprint('Specificity (FPR): {}'.format(TN / (TN + FP))) \n\nSpecificity (FPR): 0.7480314960629921\n\n\n\n#AUC is the percentage of the ROC plot that is underneath the curve:\nprint(metrics.roc_auc_score(y_test, y_pred_2))\n\n0.7970926711084192\n\n\n\n\n\nConclusion\nWith a threshold of 0.242365, the sensitivity has increased from 0.48 to 0.85, and the specificity has decreased from 0.93 to 0.75. Despite the decrease in specificity, for this project, the most important metric is sensitivity, once we want to correctly predict the patients with diabetes, the correct prediction for positive values.\nThe AUC has also increased after threshold adjustment, from 0.70 to 0.79.",
    "crumbs": [
      "Projects",
      "Project: PIMA Indians Diabetes"
    ]
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "Reference",
    "section": "",
    "text": "Livro 1\n\n\nLivro 2\n\n\nLivro 3"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome to the projects section of my portfolio, where I am pleased to share with you a selection of work that I have undertaken during my academic journey in the field of data science.\nThis page is an invitation for you to dive into the projects that not only demonstrate my technical competence but also my passion for transforming data into meaningful insights and practical solutions.\nShould any project capture your interest or if any questions arise, I encourage you to contact me. I am always available to exchange ideas, explore opportunities for collaboration, or simply to discuss the challenges and beauties found in the realm of data science.\nThank you for visiting, and I hope you find inspiration and valuable information here.",
    "crumbs": [
      "Projects"
    ]
  }
]