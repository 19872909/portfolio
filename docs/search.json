[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "+5 years of experience in Business Intelligence and Data Science working for international companies like Samsung, InBev, Bayer, and Honda, attending projects from different business areas such as marketing, operation management, and customer experience.\nData Scientist with experience working on cross-functional teams to implement effective data solutions, develop data governance policies, and leverage advanced analytics for valuable insights\nI am currently studying Behavioral Data Science at the Universitat de Barcelona, where I delve into the intersection of data analysis, psychology, and behavioral economics. This program sharpens my advanced data analysis skills using R and Python, enhancing my expertise in statistical modeling, machine learning, deep learning, and data visualization. The psychological component of the curriculum focuses on psychographics, behavior prediction through psychometrics, and behavioral modeling, empowering me to develop AI solutions tailored to human behavior. This academic journey has reinvigorated my professional enthusiasm and significantly improved my ability to apply data science in more empathetic and effective ways.\n\nBS in Electrical Engineering-Telecommunication - PUC Campinas, SP - 2011\nSpecialization in Complex Data Mining - Unicamp, SP - 2018\nSpecialization in Computer Software Engineering - Unicamp, SP - 2013\nSpecialization in Data Science (Big Data Processing and Analytics) - Mackenzie, SP - 2021\nMaster Behavioral Data Science - Barcelona University - 2024"
  },
  {
    "objectID": "index.html#my-portfolio",
    "href": "index.html#my-portfolio",
    "title": "Home",
    "section": "",
    "text": "+5 years of experience in Business Intelligence and Data Science working for international companies like Samsung, InBev, Bayer, and Honda, attending projects from different business areas such as marketing, operation management, and customer experience.\nData Scientist with experience working on cross-functional teams to implement effective data solutions, develop data governance policies, and leverage advanced analytics for valuable insights\nI am currently studying Behavioral Data Science at the Universitat de Barcelona, where I delve into the intersection of data analysis, psychology, and behavioral economics. This program sharpens my advanced data analysis skills using R and Python, enhancing my expertise in statistical modeling, machine learning, deep learning, and data visualization. The psychological component of the curriculum focuses on psychographics, behavior prediction through psychometrics, and behavioral modeling, empowering me to develop AI solutions tailored to human behavior. This academic journey has reinvigorated my professional enthusiasm and significantly improved my ability to apply data science in more empathetic and effective ways.\n\nBS in Electrical Engineering-Telecommunication - PUC Campinas, SP - 2011\nSpecialization in Complex Data Mining - Unicamp, SP - 2018\nSpecialization in Computer Software Engineering - Unicamp, SP - 2013\nSpecialization in Data Science (Big Data Processing and Analytics) - Mackenzie, SP - 2021\nMaster Behavioral Data Science - Barcelona University - 2024"
  },
  {
    "objectID": "index.html#tools",
    "href": "index.html#tools",
    "title": "Home",
    "section": "Tools",
    "text": "Tools"
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Project: PIMA Indians Diabetes",
    "section": "",
    "text": "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases.",
    "crumbs": [
      "Projects",
      "Project: PIMA Indians Diabetes"
    ]
  },
  {
    "objectID": "project1.html#correlations",
    "href": "project1.html#correlations",
    "title": "Project: PIMA Indians Diabetes",
    "section": "Correlations",
    "text": "Correlations\n\nEach square shows the correlation between the variables on each axis. Correlation ranges from -1 to +1.\n\nValues closer to 0: There is no linear trend between the two variables.\nValues closer to 1: The correlation is positive; that is as one increases so does the other and the closer to 1 the stronger this relationship is.\nValues closer to -1: One variable will decrease as the other increases\n\nThe diagonals are all 1/dark because those squares are correlating each variable to itself (so it’s a perfect correlation). For the rest the larger the number and darker the color the higher the correlation between the two variables.\n\n\n\nCorrelation matrix:\nThere is a strong positve correlation between Pregnancies and Age, Glucose and Outcome. There is a strong negative correlation between Skinthickness and Age, Pregnancies and SkinThickness, Pregnance and Insulin\n\n\nOutcome                     1.000000\nGlucose                     0.466581\nBMI                         0.292695\nAge                         0.238356\nPregnancies                 0.221898\nDiabetesPedigreeFunction    0.173844\nInsulin                     0.130548\nSkinThickness               0.074752\nBloodPressure               0.065068\nName: Outcome, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n8\n2\n197\n70\n45\n543\n30.5\n0.158\n53\n1\n\n\n11\n10\n168\n74\n0\n0\n38.0\n0.537\n34\n1\n\n\n13\n1\n189\n60\n23\n846\n30.1\n0.398\n59\n1\n\n\n14\n5\n166\n72\n19\n175\n25.8\n0.587\n51\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n749\n6\n162\n62\n0\n0\n24.3\n0.178\n50\n1\n\n\n753\n0\n181\n88\n44\n510\n43.3\n0.222\n26\n1\n\n\n754\n8\n154\n78\n32\n0\n32.4\n0.443\n45\n1\n\n\n759\n6\n190\n92\n0\n0\n35.5\n0.278\n66\n1\n\n\n761\n9\n170\n74\n31\n0\n44.0\n0.403\n43\n1\n\n\n\n\n140 rows × 9 columns\n\n\n\n\n\n\nOutcome\n1    105\n0     35\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutliers\nThere are some outliers analyzed through the distribution of data in the histograms and boxplots.\nThey following code has removed the outliers before to proceed with the ML model.\n\nIQR - Interquartile Range\nThe lower quartile corresponds with the 25th percentile and the upper quartile corresponds with the 75th percentile, so IQR = Q3 − Q1.\nThe IQR is an example of a trimmed estimator, defined as the 25% trimmed range, which enhances the accuracy of dataset statistics by dropping lower contribution, outlying points.\n\n\nPregnancies                   5.0000\nGlucose                      41.2500\nBloodPressure                18.0000\nSkinThickness                32.0000\nInsulin                     127.2500\nBMI                           9.3000\nDiabetesPedigreeFunction      0.3825\nAge                          17.0000\nOutcome                       1.0000\ndtype: float64\n\n\n\n\n(768, 9)\n\n\n\nThe interquartile range is often used to find outliers in data. Outliers here are defined as observations that fall below Q1 − 1.5 IQR or above Q3 + 1.5 IQR. In a boxplot, the highest and lowest occurring value within this limit are indicated by whiskers of the box (frequently with an additional bar at the end of the whisker) and any outliers as individual points.\n\n\n\n(639, 9)\n\n\n\n\nOutcome\n0    439\n1    200\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression ML Model\n\n\n\nStandardize features\nStandardize features by removing the mean and scaling to unit variance.\nThe standard score of a sample x is calculated as: z = (x - u) / s, where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\n\n\nLogisticRegression(max_iter=3000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=3000)\n\n\n\n\n\nML Model Assessment\n\nPrecision: Percentage of correct positive predictions relative to total positive predictions.\nRecall: Percentage of correct positive predictions relative to total actual positives.\nF1 Score: A weighted harmonic mean of precision and recall. The closer to 1, the better the model. F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.78      0.93      0.85       127\n           1       0.78      0.48      0.59        65\n\n    accuracy                           0.78       192\n   macro avg       0.78      0.70      0.72       192\nweighted avg       0.78      0.78      0.76       192\n\n\n\n\nPrecision: Out of all the patients that the model predicted would get diabetes, 78% actually did.\nRecall: Out of all the patients that actually did get diabetes, the model predicted this outcome correctly for 48% of those patients.\nF1 Score: 0.59 - Since this value is close to 1, it tells us that the model does a good job of predicting whether or not patients will get diabetes. 2 * (Precision * Recall) / (Precision + Recall) ***** 2 * (.78 * .48) / (.78 + .48)\nSupport: These values is regarding how many patients belonged to each class in the test dataset. Among the patientis in the test dataset, 127 did not get diabetes and 65 did get diabetes.\n\n\n\n0.4732423621500228\n\n\n\n\n\nROC Curve\nThe ROC curve shows the trade-off between sensitivity (or True Positve Rate) and specificity (1 – False Positive Rate).\nClassifiers that give curves closer to the top-left corner indicate a better performance\nROC/AUC does not require to set a classification threshold and it’s still useful when there is high class imbalance\n\nROC curve can help you to choose a threshold that balances sensitivity and specificity in a way that makes sense for your particular context\nYou can’t actually see the thresholds used to generate the curve on the ROC curve itself\n\n\n\n0.7030284675953968\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nPercentage of correct predictions\n\n\nAccuracy of Logistic regression model is 0.7760416666666666\n\n\n\n\nNull accuracy\nNull accuracy refers to the accuracy that could be achieved by always predicting the most frequent class in the dataset.\nIn the test set, 66% (127) of patients did not have diabetes, while 34% did. In this case, the null accuracy would be 66%, because if we always predicted “not diabetes,” we would be correct 66% of the time.\n\n\nOutcome\n0    0.661458\nName: count, dtype: float64\n\n\nAs the null accuracy is less than model accuracy, it indicates a good result.\n\nComparing the true and predicted response values\n\n\nTrue:\n\n\n(None,\n array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n        0, 0, 0]))\n\n\n\n\nPred:\n\n\n(None,\n array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n        0, 0, 1]))\n\n\n\n\n\nConfusion Matrix\nConfusion matrix allows to calculate a variety of metrics. It’s a useful for multi-class problems (more than two response classes)\n\nEvery observation in the testing set is represented in exactly one box\nIt’s 2x2 matrix because there are 2 responses classes\nThe format shown here is not universal\n\nBasic Terminology\n\nTP: correctly predict that they have diabetes\nTN: correctly predict that they do not have diabetes\nFP: incorrectly predict that they do have diabetes\nFN: incorrectly predict that they do not have diabetes\n\n\n\narray([[118,   9],\n       [ 34,  31]])\n\n\n\n\n\n\n\n\n\n\n\n\nTN: 118 patients without diabetes were correctly predicted as no diabetics\nFP: 9 patients without diabetes were incorrectly predicted as diabetics\nFN: 34 patients with diabetes were incorrectly predicted as no diabetics\nTP: 31 patients with diabetes were correctly predicted as diabetics\n\n\n\narray([[118,   9],\n       [ 34,  31]])\n\n\n\nMetrics computed from a confusion matrix\n\n\nAccuracy: 0.7760416666666666\n\n\n\n\nk-Fold Cross-Validation\nThe k-fold cross-validation procedure divides a limited dataset into k non-overlapping folds. Each of the k folds is given an opportunity to be used as a held back test set, whilst all other folds collectively are used as a training dataset. A total of k models are fit and evaluated on the k hold-out test sets and the mean performance is reported.\n\n\nAccuracy: 0.784 (0.033)\n\n\n\n\nError: 0.22395833333333334\n\n\n\n\nSensitivity (TPR): 0.47692307692307695\n\n\n\n\nSpecificity (FPR): 0.9291338582677166\n\n\n\n\nPrecision: 0.775\n\n\n\n\n\nResult\nThe choice of metrics depends on the business objective.\nFor this project, sensibility (False Negative) is the metric most important, since predicting diabetics as no diabetics is the worst expected error. This may imply no further investigations and consequently no treatment of the disease.\nSo the better result is to have a Sensitivity (The correct prediction for positive values) result higher than a Specificity (The correct prediction for negative values) result.\nThe error in predicting a healthy patient as a diabetic patient is more acceptable than the opposite.\n\n\nAdjusting the classification threshold\n\n\narray([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n\n\n\n\narray([[0.83358342, 0.16641658],\n       [0.80138151, 0.19861849],\n       [0.95231282, 0.04768718],\n       [0.87348851, 0.12651149],\n       [0.67158939, 0.32841061],\n       [0.38292382, 0.61707618],\n       [0.93226643, 0.06773357],\n       [0.5219636 , 0.4780364 ],\n       [0.92950252, 0.07049748],\n       [0.69118097, 0.30881903]])\n\n\n\n\narray([0.16641658, 0.19861849, 0.04768718, 0.12651149, 0.32841061,\n       0.61707618, 0.06773357, 0.4780364 , 0.07049748, 0.30881903])\n\n\n\n\n\n\n\n\n\n\n\n\nFinding the best threshold for predicting diabetes and to increase the sensitivity of the classifier\nThreshold of 0.5 is used by default (for binary problems) to convert predicted possibilities into class predictions Threshold can be adjusted to increase sensitivity or specificity Sensitivity and specificity have an inverse relationship\n\n\nBest Threshold=0.242365, G-Mean=0.796\n\n\n\n\nBest Threshold=0.242365\n\n\n\n\nSensitivity: 0.47692307692307695\nSpecificity: 0.9291338582677166\n\n\n\n\nSensitivity: 0.7230769230769231\nSpecificity: 0.7795275590551181\n\n\n\n\nSensitivity: 0.8461538461538461\nSpecificity: 0.7480314960629921\n\n\n\n\narray([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n\n\n\n\narray([0., 0., 0., 0., 1., 1., 0., 1., 0., 1.])\n\n\n\n\narray([[118,   9],\n       [ 34,  31]])\n\n\n\n\narray([[95, 32],\n       [10, 55]])\n\n\n\n\nSensitivity (TPR): 0.8461538461538461\n\n\n\n\nSpecificity (FPR): 0.7480314960629921\n\n\n\n\n0.7970926711084192\n\n\n\n\n\nConclusion\nWith a threshold of 0.242365, the sensitivity has increased from 0.48 to 0.85, and the specificity has decreased from 0.93 to 0.75. Despite the decrease in specificity, for this project, the most important metric is sensitivity, once we want to correctly predict the patients with diabetes, the correct prediction for positive values.\nThe AUC has also increased after threshold adjustment, from 0.70 to 0.79.",
    "crumbs": [
      "Projects",
      "Project: PIMA Indians Diabetes"
    ]
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "Reference",
    "section": "",
    "text": "Livro 1\n\n\nLivro 2\n\n\nLivro 3"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome to the projects section of my portfolio, where I am pleased to share with you a selection of work that I have undertaken during my academic journey in the field of data science.\nThis page is an invitation for you to dive into the projects that not only demonstrate my technical competence but also my passion for transforming data into meaningful insights and practical solutions.\nShould any project capture your interest or if any questions arise, I encourage you to contact me. I am always available to exchange ideas, explore opportunities for collaboration, or simply to discuss the challenges and beauties found in the realm of data science.\nThank you for visiting, and I hope you find inspiration and valuable information here.",
    "crumbs": [
      "Projects"
    ]
  }
]